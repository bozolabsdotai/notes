In this tutorial we'll try configure and run some existing real time processing examples on Horton Sandbox 2.2 using 
Storm,Kafka,Hbase. Please find below links which we are trying to fix:
Tutorial 1: http://hortonworks.com/hadoop-tutorial/simulating-transporting-realtime-events-stream-apache-kafka/
Tutorial 2: http://hortonworks.com/hadoop-tutorial/ingesting-processing-real-time-events-apache-storm/
Tutorial 3: http://hortonworks.com/hadoop-tutorial/real-time-data-ingestion-hbase-hive-using-storm-bolt/

The tutorials need to be executed in sequence but if you try to do the same it will simply not work. Tutorials and code
are missing some important settings before you start. Please find them below.

0. After downloadin the HortonSandbox VM and importing and booting the VM. One can use the instructions present in
https://github.com/upendrasingh1/notes/blob/master/hortonsandboxdevsetup . Goto terminal and modify /etc/hosts
using vim to look like below:

127.0.0.1               localhost.localdomain localhost
X.X.X.X     sandbox.hortonworks.com sandbox ambari.hortonworks.com

Then network restart or VM reboot to reflect the changes above:
#service network restart
1. In Step 5 of Tutorial 1 after below steps
#mkdir /opt/TruckEvents  
#cd /opt/TruckEvents  
#wget http://hortonassets.s3.amazonaws.com/mda/Tutorials-master.zip  
#unzip Tutorials-master.zip
2. There is a runtime dependency missing in pom.xml file. Add following dependency in pom.xml:
        <dependency>
            <groupId>xerces</groupId>
            <artifactId>xercesImpl</artifactId>
            <version>2.11.0</version>
        </dependency>
3. In package com.hortonworks.tutorials.tutorial3: File TruckEventProcessingTopology.java inside function
buildAndSubmit() comment out configureHDFSBolt(builder) function call. This function is completely broken. This will
ensure hive and hdfs part of Tutorial 3 will not be invoked but HBase part will work fine.
4. Goto Ambari Console 127.0.0.1:8080 using admin/admin. Select HBase->Configs->Advanced HBase Site
add user storm and root to hbase.superuser property. It should look like hbase.superuser hbase,root,storm

After above steps we can start executing the tutorials in the sequence but whereever Producer code is invoked 
using below command in all three tutorials 

java -cp target/Tutorial-1.0-SNAPSHOT.jar com.hortonworks.tutorials.tutorial1.TruckEventsProducer localhost:9092 localhost:2181 &

use this command
java -cp target/Tutorial-1.0-SNAPSHOT.jar com.hortonworks.tutorials.tutorial1.TruckEventsProducer sandbox.hortonworks.com:6667 sandbox.hortonworks.com:2181 &


For assemblylineevent:
Open two command line terminal using Applications->System Tools->Terminal

Generating events:
On one terminal run below commands:
#cd NetBeansProjects/AssemblyLineDataEventProcessing
#java -cp target/AssemblyLineDataEventProcessing-1.0-SNAPSHOT.jar com.mycompany.assemblylinedataeventprocessing.AssemblyLineEventSimulator  sandbox.hortonworks.com:6667 sandbox.hortonworks.com:2181 &

Depolying Topology:
On another terminal run below commands:
#cd NetBeansProjects/AssemblyLineDataEventProcessing
#storm jar target/AssemblyLineDataEventProcessing-1.0-SNAPSHOT.jar com.mycompany.assemblylinedataeventprocessing.AssemblyLineEventProcessingTopology

To create a topic in kafka:
bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic assemblylineevent

To delete a topic in kafka:
bin/kafka-run-class.sh kafka.admin.DeleteTopicCommand --zookeeper localhost:2181 --topic assemblylineevent



Installing Storm on Cloudera Quickstart VM:

1.ZeroMQ installation:
 Edit vim /etc/yum.repos.d/zeromq.repo
 
 [home_fengshuo_zeromq]
name=The latest stable of zeromq builds (CentOS_CentOS-6)
type=rpm-md
baseurl=http://download.opensuse.org/repositories/home:/fengshuo:/zeromq/CentOS_CentOS-6/
gpgcheck=1
gpgkey=http://download.opensuse.org/repositories/home:/fengshuo:/zeromq/CentOS_CentOS-6/repodata/repomd.xml.key
enabled=1

2. Install Java Binding for ZeroMQ
#cd /usr/local/src
#git clone https://github.com/zeromq/jzmq
#cd jzmq
#./autogen
#./configure
#make
#make install

3.Installing Storm

#Downloaded Storm from http://storm-project.net/downloads.html
#Unzipped the tar file under /usr/share
#Created a file storm.sh under /etc/profile.d/

    added folowing line to the storm.sh export PATH=${PATH}:/usr/share/apache-storm-0.9.5/bin:

#Modified the /usr/share/apache-storm-0.9.5/conf/storm.yaml to add following lines

    storm.zookeeper.servers:
    – “127.0.0.1”
    nimbus.host: “localhost”
    storm.local.dir: “/var/storm”
    supervisor.slots.ports:
    – 6700
    – 6701
    – 6702
    – 6703
    ui.port: 8772

#Start Storm

    I installed storm as root user…so had to launch all daemons as root. This was a bad choice. I should have created a Storm user and gave permission to /usr/share/storm-0.9.0.1 and /var/storm
    Anyway, I launched three terminals, on each terminal I did su - root
        Terminal 1 : storm nimbus
            Terminal 2 : storm supervisor
                Terminal 3 : storm ui
            Then went to http://localhost:8772 to view storm UI

Good link: http://www.michael-noll.com/tutorials/running-multi-node-storm-cluster/

Making PostGres Columnar:

1. Install postgres using following link: http://www.unixmen.com/postgresql-9-4-released-install-centos-7/
   Used CentOS 6.5 for installation.
   
2. Follow steps available in https://github.com/citusdata/cstore_fdw


Realtime OpenTSDB + Phoenix + Spark:

https://gist.github.com/abajwa-hw/6ebacc472a33334945bd

Cassandra + SPARK(single machine):
http://blog.knoldus.com/2015/06/23/apache-spark-cassandra-basic-steps-to-install-and-configure-cassandra-and-use-it-with-apache-spark-with-example/

Kafka C++ Producer:
https://github.com/bitbouncer/csi-kafka

Kafka + Avro + Storm:
https://github.com/ransilberman/avro-kafka-storm

Amazon Kinessis + Storm:
https://github.com/awslabs/kinesis-storm-spout

Storm + s3:
https://github.com/wurstmeister/storm-s3

Storm + wasb:
https://github.com/hdinsight/hdinsight-storm-examples/blob/master/IotExample/iot/src/main/java/com/microsoft/hdinsight/storm/examples/IotTopology.java#L104

Data Pipeline:
AWS: AWS Data Pipeline
Getting Started Developer Guide
http://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/what-is-datapipeline.html
http://docs.aws.amazon.com/ElasticMapReduce/latest/DeveloperGuide/emr-hive.html

Azure: Data Factory
Step1: https://azure.microsoft.com/en-in/documentation/articles/data-factory-create-datasets/
Step2: https://azure.microsoft.com/en-in/documentation/articles/data-factory-create-pipelines/
Step3: https://azure.microsoft.com/en-in/documentation/articles/data-factory-data-movement-activities/
Step4: https://azure.microsoft.com/en-in/documentation/articles/data-factory-data-transformation-activities/
       (In Step4 select Hive u will reach Step5)
Step5: https://azure.microsoft.com/en-in/documentation/articles/data-factory-hive-activity/
Step6: https://azure.microsoft.com/en-in/documentation/articles/data-factory-monitor-manage-pipelines/

Azure Storm Cluster:
https://azure.microsoft.com/en-in/documentation/articles/hdinsight-apache-storm-tutorial-get-started/

Storm JDBC Topology:
https://github.com/apache/storm/tree/master/external/storm-jdbc/src/test/java/org/apache/storm/jdbc/topology
